---
slug: running-GPTJ-on-M1-Macbook
title: Running GPTJ (EleutherAI/gpt-j-6B) on an M1 Macbook
date: 2023-02-24
author: bebrws
tags:
  - Machine Learning
  - AI
  - ML
  - GPTJ
  - GPT 
  - GPT-3
  - GPT-J
  - GPT-J-6B
  - Happy Transformer
  - HuggingFace
---

# Running GPTJ (EleutherAI/gpt-j-6B) on an M1 Macbook

So I am blown away by how helpful GitHub CoPilot and ChatGPT from OpenAI is.

And I have no previous experience with Machine Learning really. At least past a surface level.

I was able to get some simple tests setup on a GPU instance on AWS. But I wanted to try it out on my M1 Macbook.

And I just read that PyTorch JUST added support for M1 Macbooks. So I figured I would give it a shot.

Below is the documentation I used to get it working.

NOTE: 

    Most of this is coming from this [blog post](https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/)
    I will be updating the same information and it will be more up to date in the [repository](https://github.com/bebrws/gptj6b-happy-on-m1/)


## Sources and what I followed to get here:
```
https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/
https://github.com/huggingface/transformers/blob/ba0e370dc1713b0ddd9b1be0ac31ef1fdc7bdf76/docs/source/en/model_doc/gptj.mdx?plain=1#L62
```

## Setup:

PreRequirements:
* Install xcode and brew
* MiniForge3 (donâ€™t use Anaconda) -> download the arm64 sh from GitHub - https://github.com/conda-forge/miniforge#download
* Make sure you have the latest rust installed via rustup
* You will need to have brew installed llvm, cmake, and pkgconfig as well

Steps:

```
# set up conda for fish
export PATH="$HOME/miniforge3/bin/:$PATH"
conda init zsh

# create and use a new env
conda create --name tf python=3.9
conda activate tf

# install tensorflow deps
conda install -c apple tensorflow-deps
# base tensorflow + metal plugin
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal

# install jupyter, pandas and whatnot
conda install -c conda-forge -y pandas jupyter

mkdir -p Projects/lab/tfsetup && cd Projects/lab/tfsetup
rm -rf tokenizers
git clone https://github.com/huggingface/tokenizers
cd tokenizers/bindings/python
# compile tokenizers - should be pretty fast on your m1
pip install setuptools_rust
# install tokenizers
python setup.py install

# install transformers using pip
pip install git+https://github.com/huggingface/transformers
pip install numpy --upgrade --ignore-installed

arch -arm64 brew install llvm
arch -arm64 brew install cmake
arch -arm64 brew install pkgconfig


cd ../../../../../../ # Back to root dir

# run the test code
export PYTORCH_ENABLE_MPS_FALLBACK=1
python gptjex.py


```

If you run now you will see errors about MPS not supporting different things:
```
RuntimeError: MPS does not support cumsum op with int64 input
```

To get around this:
```
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

## Testing install

```
import tensorflow as tf

print("GPUs: ", len(tf.config.experimental.list_physical_devices('GPU')))
```

Run the actual code!!!:
```
#!/usr/bin/env python3
# export PYTORCH_ENABLE_MPS_FALLBACK=1

from transformers import GPTJForCausalLM, AutoTokenizer
import torch
import os
model = GPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")
print("Created model")
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
print("Created tokenizer")
prompt = (
    "How do I create a thread in GoLang?"
)
tougherprompt = (
    """
function reverse_array() {
    # Usage: reverse_array "array"
    shopt -s extdebug
    f()(printf '%s\\n' "${BASH_ARGV[@]}"); f "$@"
    shopt -u extdebug
}

# Please explain how the bash shell code above works.
    """
)

input_ids = tokenizer(prompt, return_tensors="pt").input_ids
print("Got input_ids from tokenizer")
gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=2048,
    pad_token_id=tokenizer.eos_token_id
)
print("Generated tokens")
gen_text = tokenizer.batch_decode(gen_tokens)[0]
print("Got gen_text from tokenizer.batch_decode\n\n\n")
print(gen_text)
os.system("say done")

```

### Possible issues:
If you get an error about the device number not being valid from the model.generate() call, try setting the device to new pytorch mps device:
```
# Edit the code for happytransformer
code /opt/homebrew/anaconda3/envs/tf2/lib/python3.9/site-packages/happytransformer/happy_generation.py
```

On line 58 or so there is:
```
device_number = detect_cuda_device_number()
``` 
I changed this to:
```
device_number = torch.device("mps")
```
I also had to make sure I had:
```
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

## Other things I did that might have helped

Install nightly:
```
pip install --pre torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu
```

```
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly

```

## Testing PyTorch + CUDA

```
python -c 'import torch; print torch.cuda.is_available()'\
```