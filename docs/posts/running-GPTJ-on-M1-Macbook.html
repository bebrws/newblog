<!DOCTYPE html><html><head><script type="text/javascript" class="jsx-707554062">
          
          function loadBevySnake() {
            setTimeout(function() {
            var element = document.getElementById('bevy-portal');
                  if (element) {
                    console.log('Element with id "bevy-portal" exists. Adding script tag.');

                  var script = document.createElement('script');
                  script.type = 'module';
                  script.src = '../static/load-bevy-snake.js';
                  document.head.appendChild(script);
            } else {
                    console.log('Element with id "bevy-portal" does not exist.');
                    loadBevySnake();
            }
          }, 500);
        }

        loadBevySnake();
      </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-154810525-2" class="jsx-707554062"></script><script type="text/javascript" class="jsx-707554062">
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'UA-154810525-2');
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-BZKS5D8HPF" class="jsx-707554062"></script><script type="text/javascript" class="jsx-707554062">
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-BZKS5D8HPF');
        </script><meta charSet="utf-8" class="jsx-707554062"/><meta name="viewport" content="width=device-width, initial-scale=1" class="jsx-707554062"/><link rel="manifest" href="/site.webmanifest" class="jsx-707554062"/><link rel="apple-touch-icon" href="/icon.png" class="jsx-707554062"/><meta name="theme-color" content="#fff" class="jsx-707554062"/><title>Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook | Brad Barrows&#x27; Blog</title><meta name="description" content="Brad Barrows&#x27; Blog"/><link rel="canonical" href="https://bbarrows.com/posts/running-GPTJ-on-M1-Macbook"/><meta property="og:site_name" content="Brad Barrows&#x27; Blog"/><meta property="og:url" content="https://bbarrows.com/posts/running-GPTJ-on-M1-Macbook"/><meta property="og:title" content="Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook | Brad Barrows&#x27; Blog"/><meta property="og:description" content="Brad Barrows&#x27; Blog"/><meta property="og:image" content="https://bbarrows.com/og_image.png"/><meta property="og:type" content="article"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":"https://bbarrows.com/posts/running-GPTJ-on-M1-Macbook","headline":"Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook","datePublished":"2023-02-26T00:00:00-08:00","author":"Brad Barrows","description":""}</script><meta name="next-head-count" content="20"/><link rel="preload" href="/_next/static/css/8e11f38ccf46f6277e39.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e11f38ccf46f6277e39.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e884486185169a1848b2.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e884486185169a1848b2.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-f47d69457824065d04c3.js" defer=""></script><script src="/_next/static/chunks/framework-895f067827ebe11ffe45.js" defer=""></script><script src="/_next/static/chunks/main-d32200c037819e7e8fa0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eae7af63eb651790e0a7.js" defer=""></script><script src="/_next/static/chunks/962-b2e3b32260744832b5cf.js" defer=""></script><script src="/_next/static/chunks/295-d2a92dd57ddb2277243a.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpost%5D-554a7f2db08ad8e20c9d.js" defer=""></script><script src="/_next/static/jhgFwLEJbxGrw0_D-f_Gp/_buildManifest.js" defer=""></script><script src="/_next/static/jhgFwLEJbxGrw0_D-f_Gp/_ssgManifest.js" defer=""></script><style id="__jsx-3058196896">.container.jsx-3058196896{position:fixed;width:38px;height:38px;cursor:pointer;top:1rem;left:1.25rem;z-index:2;background-color:rgba(255,255,255,0.7);}.meat.jsx-3058196896{position:absolute;width:28px;height:2px;background:#222;top:calc(50% - 2px / 2);left:calc(50% - 28px / 2);-webkit-transition:all 150ms ease-in;transition:all 150ms ease-in;}.meat-1.jsx-3058196896{-webkit-transform:translateY(-10px);-ms-transform:translateY(-10px);transform:translateY(-10px);}.meat-2.jsx-3058196896{width:calc(28px - 6px);}.meat-3.jsx-3058196896{-webkit-transform:translateY(10px);-ms-transform:translateY(10px);transform:translateY(10px);}.active.jsx-3058196896 .meat-1.jsx-3058196896{-webkit-transform:rotate(45deg);-ms-transform:rotate(45deg);transform:rotate(45deg);}.active.jsx-3058196896 .meat-2.jsx-3058196896{opacity:0;}.active.jsx-3058196896 .meat-3.jsx-3058196896{-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);}@media (min-width:769px){.container.jsx-3058196896{display:none;}}</style><style id="__jsx-4138737674">.container.jsx-4138737674{width:0;}ul.jsx-4138737674{opacity:0;width:100%;height:100vh;text-align:right;list-style:none;margin:0;padding:0;position:fixed;top:0;background-color:#fff;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;z-index:1;-webkit-transform:translateY(100%);-ms-transform:translateY(100%);transform:translateY(100%);-webkit-transition:opacity 200ms;transition:opacity 200ms;}.active.jsx-4138737674 ul.jsx-4138737674{opacity:1;-webkit-transform:translateY(0);-ms-transform:translateY(0);transform:translateY(0);}li.jsx-4138737674{margin-bottom:1.75rem;font-size:2rem;padding:0 1.5rem 0 0;}li.jsx-4138737674:last-child{margin-bottom:0;}.active.jsx-4138737674{color:#222;}@media (min-width:769px){.container.jsx-4138737674{width:7rem;display:block;}ul.jsx-4138737674{opacity:1;width:7rem;top:auto;display:block;-webkit-transform:translateY(0);-ms-transform:translateY(0);transform:translateY(0);}li.jsx-4138737674{font-size:1rem;padding:0;}}</style><style id="__jsx-3674465219">span.jsx-3674465219{color:#9b9b9b;}</style><style id="__jsx-255066894">a.jsx-255066894{display:inline-block;}a.jsx-255066894:not(:last-child){margin-right:2em;}</style><style id="__jsx-3618008669">p.jsx-3618008669{font-size:0.75rem;text-align:center;}</style><style id="__jsx-4153080612">.container.jsx-3387257903{display:block;max-width:60rem;width:100%;margin:0 auto;padding:0 1.5rem;box-sizing:border-box;z-index:0;}.metadata.jsx-3387257903 div.jsx-3387257903{display:inline-block;margin-right:0.5rem;}article.jsx-3387257903{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;}h1.jsx-3387257903{margin:0 0 0.5rem;font-size:2.25rem;}.tag-list.jsx-3387257903{list-style:none;text-align:right;margin:1.75rem 0 0 0;padding:0;}.tag-list.jsx-3387257903 li.jsx-3387257903{display:inline-block;margin-left:0.5rem;}.social-list.jsx-3387257903{margin-top:3rem;text-align:center;}@media (min-width:769px){.container.jsx-3387257903{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}}</style><style id="__jsx-1699270818">.token.comment,.token.prolog,.token.doctype,.token.cdata,.token.plain-text{color:#6a737d;}.token.atrule,.token.attr-value,.token.keyword,.token.operator{color:#d73a49;}.token.property,.token.tag,.token.boolean,.token.number,.token.constant,.token.symbol,.token.deleted{color:#22863a;}.token.selector,.token.attr-name,.token.string,.token.char,.token.builtin,.token.inserted{color:#032f62;}.token.function,.token.class-name{color:#6f42c1;}.language-jsx .token.punctuation,.language-jsx .token.tag .token.punctuation,.language-jsx .token.tag .token.script,.language-jsx .token.plain-text{color:#24292e;}.language-jsx .token.tag .token.attr-name{color:#6f42c1;}.language-jsx .token.tag .token.class-name{color:#005cc5;}.language-jsx .token.tag .token.script-punctuation,.language-jsx .token.attr-value .token.punctuation:first-child{color:#d73a49;}.language-jsx .token.attr-value{color:#032f62;}.language-jsx span[class="comment"]{color:pink;}.language-html .token.tag .token.punctuation{color:#24292e;}.language-html .token.tag .token.attr-name{color:#6f42c1;}.language-html .token.tag .token.attr-value,.language-html .token.tag .token.attr-value .token.punctuation:not(:first-child){color:#032f62;}.language-css .token.selector{color:#6f42c1;}.language-css .token.property{color:#005cc5;}</style><style id="__jsx-707554062">.root.jsx-707554062{display:block;padding:4rem 0;box-sizing:border-box;height:100%;}main.jsx-707554062{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100%;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}@media (min-width:769px){.root.jsx-707554062{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;}main.jsx-707554062{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;}}</style></head><body><div id="__next"><div class="jsx-707554062 root"><nav class="jsx-707554062"><div class="jsx-3058196896 container "><div class="jsx-3058196896 meat meat-1"></div><div class="jsx-3058196896 meat meat-2"></div><div class="jsx-3058196896 meat meat-3"></div></div><div class="jsx-4138737674 container "><ul class="jsx-4138737674"><li class="jsx-4138737674"><a class="jsx-4138737674 " href="/">about</a></li><li class="jsx-4138737674"><a class="jsx-4138737674 " href="/projects">projects</a></li><li class="jsx-4138737674"><a class="jsx-4138737674 active" href="/posts">blog</a></li></ul></div></nav><main class="jsx-707554062"><div class="jsx-3387257903 container"><article class="jsx-3387257903"><header class="jsx-3387257903"><h1 class="jsx-3387257903">Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook</h1><div class="jsx-3387257903 metadata"><div class="jsx-3387257903"><time dateTime="2023-02-26T00:00:00-08:00" class="jsx-3674465219"><span class="jsx-3674465219">February 26, 2023</span></time></div><div class="jsx-3387257903"><span class="jsx-3674465219">Brad Barrows</span></div></div></header><div class="jsx-3387257903 content_content__1yuPc"><div><h1>Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook</h1><p><img src="/static/gpt.gif" alt="Running GPT locally on a M1 Macbook"/>
<em>Actual runtime on my 64GB RAM Apple M1 Max Macbook was 15 minutes. The first answer I got back from this prompt was actually more concise but the answer in this video is actually a better, more thorough one</em></p><h2>ML - CoPilot and ChatGPT</h2><p>It is amazing how helpful GitHub CoPilot / ChatGPT can be sometimes. And I have no previous experience with Machine Learning really. At least past a surface level.</p><p>I wanted to learn more and really my goal is to create something like <a href="https://github.com/arc53/DocsGPT">DocsGPT</a> which will analyze your codebase&#x27;s code and documentation and be able to provide in depth answers specific to your codebase. I tried some simple tests setup on a GPU instance on AWS but this was going to be very expensive and I of course would prefer to get this all running locally on my M1 Macbook.</p><h2>PyTorch has M1 Macbook Support!</h2><p>After looking around I found that PyTorch JUST recently added support for M1 Macbooks.</p><p>Below is the documentation I used to get it working.</p><p>NOTE:
Most of this is coming from (copied from) this <a href="https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/">blog post</a>
I will be updating the same information and it will be more up to date in the <a href="https://github.com/bebrws/gptj6b-happy-on-m1/">repository</a>
I just had to do a few different things which I document toward the end to get this working on my machine. Most importantly, this was failing to run due to a lack of CUDA support. To fix this I needed to install pytorch-nightly (which can be done a few ways). And then PyTorch doesn&#x27;t supprt the M1 &quot;mps&quot; device so I needed to set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1
All of this is documented below.</p><h2>Sources and What I Followed To Get Here:</h2><pre><code>https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/
https://github.com/huggingface/transformers/blob/ba0e370dc1713b0ddd9b1be0ac31ef1fdc7bdf76/docs/source/en/model_doc/gptj.mdx?plain=1#L62
</code></pre><h2>Setup:</h2><p>PreRequirements:</p><ul><li>Install xcode and brew</li><li>MiniForge3 (don’t use Anaconda) -&gt; download the arm64 sh from GitHub - <a href="https://github.com/conda-forge/miniforge#download">https://github.com/conda-forge/miniforge#download</a></li><li>Make sure you have the latest rust installed via rustup</li><li>You will need to have brew installed llvm, cmake, and pkgconfig as well</li></ul><h3>Steps:</h3><pre><code># set up conda for fish
export PATH=&quot;$HOME/miniforge3/bin/:$PATH&quot;
conda init zsh

# create and use a new env
conda create --name tf python=3.9
conda activate tf

# install tensorflow deps
conda install -c apple tensorflow-deps
# base tensorflow + metal plugin
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal

# install jupyter, pandas and whatnot
conda install -c conda-forge -y pandas jupyter

mkdir -p Projects/lab/tfsetup &amp;&amp; cd Projects/lab/tfsetup
rm -rf tokenizers
git clone https://github.com/huggingface/tokenizers
cd tokenizers/bindings/python
# compile tokenizers - should be pretty fast on your m1
pip install setuptools_rust
# install tokenizers
python setup.py install

# install transformers using pip
pip install git+https://github.com/huggingface/transformers
pip install numpy --upgrade --ignore-installed

arch -arm64 brew install llvm
arch -arm64 brew install cmake
arch -arm64 brew install pkgconfig


cd ../../../../../../ # Back to root dir

# run the test code
export PYTORCH_ENABLE_MPS_FALLBACK=1
python gptjex.py


</code></pre><p>If you run now you will see errors about MPS not supporting different things:</p><pre><code>RuntimeError: MPS does not support cumsum op with int64 input
</code></pre><p>To get around this:</p><pre><code>export PYTORCH_ENABLE_MPS_FALLBACK=1
</code></pre><h3>Testing Install:</h3><pre><code>import tensorflow as tf

print(&quot;GPUs: &quot;, len(tf.config.experimental.list_physical_devices(&#x27;GPU&#x27;)))
</code></pre><h3>Run the actual code:</h3><pre><code>#!/usr/bin/env python3
# export PYTORCH_ENABLE_MPS_FALLBACK=1

from transformers import GPTJForCausalLM, AutoTokenizer
import torch
import os
model = GPTJForCausalLM.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)
print(&quot;Created model&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)
print(&quot;Created tokenizer&quot;)
prompt = (
    &quot;How do I create a thread in GoLang?&quot;
)

input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids
print(&quot;Got input_ids from tokenizer&quot;)
gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=2048,
    pad_token_id=tokenizer.eos_token_id
)
print(&quot;Generated tokens&quot;)
gen_text = tokenizer.batch_decode(gen_tokens)[0]
print(&quot;Got gen_text from tokenizer.batch_decode\n\n\n&quot;)
print(gen_text)
os.system(&quot;say done&quot;)

</code></pre><h3>Possible issues:</h3><p>If you get an error about the device number not being valid from the model.generate() call, try setting the device to new pytorch mps device:</p><pre><code># Edit the code for happytransformer
code /opt/homebrew/anaconda3/envs/tf2/lib/python3.9/site-packages/happytransformer/happy_generation.py
</code></pre><p>On line 58 or so there is:</p><pre><code>device_number = detect_cuda_device_number()
</code></pre><p>I changed this to:</p><pre><code>device_number = torch.device(&quot;mps&quot;)
</code></pre><p>I also had to make sure I had:</p><pre><code>export PYTORCH_ENABLE_MPS_FALLBACK=1
</code></pre><h3>Other Things I Did That Might Have Helped:</h3><p>Install nightly.
I believe this is what finally worked:</p><pre><code>conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly

</code></pre><p>Although the documentation (I believe) and what I tried before doing the command above was:</p><pre><code>pip install --pre torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu
</code></pre><h3>Testing PyTorch + CUDA:</h3><pre><code>python -c &#x27;import torch; print torch.cuda.is_available()&#x27;\
</code></pre></div></div></article><footer class="jsx-3387257903"><div class="jsx-3387257903 social-list"><div class="jsx-255066894"><a title="LinkedIn" href="https://www.linkedin.com/in/bbarrows/}" target="_blank" rel="noopener" class="jsx-255066894"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="#222"><path fill="#444" d="M5.453 11.099v15.484H.297V11.099h5.156zm.328-4.781q.016 1.141-.789 1.906t-2.117.766h-.031q-1.281 0-2.063-.766T0 6.318q0-1.156.805-1.914t2.102-.758 2.078.758.797 1.914zM24 17.708v8.875h-5.141v-8.281q0-1.641-.633-2.57t-1.977-.93q-.984 0-1.648.539t-.992 1.336q-.172.469-.172 1.266v8.641H8.296q.031-6.234.031-10.109t-.016-4.625l-.016-.75h5.141v2.25h-.031q.312-.5.641-.875t.883-.812 1.359-.68 1.789-.242q2.672 0 4.297 1.773t1.625 5.195z"></path></svg></a><a title="GitHub" href="https://github.com/bebrws/" target="_blank" rel="noopener" class="jsx-255066894"><svg viewBox="0 0 11 11" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2" width="24" height="24" fill="#222"><path d="M10.5 5.63a4.992 4.992 0 01-.977 3.016A5.018 5.018 0 016.998 10.5c-.12.022-.207.007-.263-.047a.267.267 0 01-.083-.2v-1.41c0-.432-.116-.748-.347-.948.253-.027.481-.067.683-.12.203-.054.412-.14.627-.26.216-.121.396-.27.54-.445.145-.176.263-.41.354-.702.091-.291.137-.626.137-1.005a1.95 1.95 0 00-.527-1.376c.164-.405.147-.86-.053-1.363-.125-.04-.305-.015-.54.074a3.59 3.59 0 00-.614.294l-.254.16a4.727 4.727 0 00-1.28-.174c-.44 0-.867.058-1.28.174a5.931 5.931 0 00-.284-.18 3.995 3.995 0 00-.557-.258c-.254-.1-.445-.13-.574-.09-.195.504-.211.958-.046 1.363a1.95 1.95 0 00-.527 1.376c0 .379.045.713.136 1.002.092.29.208.523.35.702.143.178.322.327.537.447.216.12.425.207.627.26.203.054.43.094.684.12-.178.161-.287.39-.327.689a1.32 1.32 0 01-.3.1 1.885 1.885 0 01-.38.034A.785.785 0 013 8.573a1.177 1.177 0 01-.37-.418 1.077 1.077 0 00-.324-.347.973.973 0 00-.33-.16l-.133-.02a.432.432 0 00-.194.03c-.035.02-.046.045-.033.077a.487.487 0 00.147.174l.046.033c.098.044.195.129.29.254.096.124.166.238.21.34l.067.154c.058.17.156.306.294.41.138.105.287.172.447.201.16.029.314.045.463.047a1.96 1.96 0 00.37-.024l.154-.026a62.026 62.026 0 00.007.955c0 .08-.03.147-.087.2-.058.054-.147.07-.267.047a5.018 5.018 0 01-2.524-1.854A4.992 4.992 0 01.255 5.63c0-.93.23-1.789.687-2.575a5.103 5.103 0 011.865-1.867A5.005 5.005 0 015.377.5c.93 0 1.787.23 2.572.688a5.103 5.103 0 011.864 1.867c.458.786.687 1.645.687 2.575z" fill="#444" fill-rule="nonzero"></path></svg></a></div></div><p class="jsx-3618008669">© 2021</p></footer></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook","dateString":"2023-02-26","slug":"running-GPTJ-on-M1-Macbook","description":"","tags":["Machine Learning","AI","ML","GPTJ","GPT","GPT-3","GPT-J","GPT-J-6B","Happy Transformer","HuggingFace"],"author":"bebrws","source":{"compiledSource":"\"use strict\";\n\nvar _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i \u003c arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i \u003c sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) \u003e= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i \u003c sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) \u003e= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar layoutProps = {};\nvar MDXLayout = \"wrapper\";\nfunction MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/static/gpt.gif\",\n    \"alt\": \"Running GPT locally on a M1 Macbook\"\n  }), \"\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Actual runtime on my 64GB RAM Apple M1 Max Macbook was 15 minutes. The first answer I got back from this prompt was actually more concise but the answer in this video is actually a better, more thorough one\")), mdx(\"h2\", null, \"ML - CoPilot and ChatGPT\"), mdx(\"p\", null, \"It is amazing how helpful GitHub CoPilot / ChatGPT can be sometimes. And I have no previous experience with Machine Learning really. At least past a surface level.\"), mdx(\"p\", null, \"I wanted to learn more and really my goal is to create something like \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/arc53/DocsGPT\"\n  }, \"DocsGPT\"), \" which will analyze your codebase's code and documentation and be able to provide in depth answers specific to your codebase. I tried some simple tests setup on a GPU instance on AWS but this was going to be very expensive and I of course would prefer to get this all running locally on my M1 Macbook.\"), mdx(\"h2\", null, \"PyTorch has M1 Macbook Support!\"), mdx(\"p\", null, \"After looking around I found that PyTorch JUST recently added support for M1 Macbooks.\"), mdx(\"p\", null, \"Below is the documentation I used to get it working.\"), mdx(\"p\", null, \"NOTE:\\nMost of this is coming from (copied from) this \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/\"\n  }, \"blog post\"), \"\\nI will be updating the same information and it will be more up to date in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/bebrws/gptj6b-happy-on-m1/\"\n  }, \"repository\"), \"\\nI just had to do a few different things which I document toward the end to get this working on my machine. Most importantly, this was failing to run due to a lack of CUDA support. To fix this I needed to install pytorch-nightly (which can be done a few ways). And then PyTorch doesn't supprt the M1 \\\"mps\\\" device so I needed to set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1\\nAll of this is documented below.\"), mdx(\"h2\", null, \"Sources and What I Followed To Get Here:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/\\nhttps://github.com/huggingface/transformers/blob/ba0e370dc1713b0ddd9b1be0ac31ef1fdc7bdf76/docs/source/en/model_doc/gptj.mdx?plain=1#L62\\n\")), mdx(\"h2\", null, \"Setup:\"), mdx(\"p\", null, \"PreRequirements:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Install xcode and brew\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"MiniForge3 (don\\u2019t use Anaconda) -\u003e download the arm64 sh from GitHub - \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/conda-forge/miniforge#download\"\n  }, \"https://github.com/conda-forge/miniforge#download\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Make sure you have the latest rust installed via rustup\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You will need to have brew installed llvm, cmake, and pkgconfig as well\")), mdx(\"h3\", null, \"Steps:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"# set up conda for fish\\nexport PATH=\\\"$HOME/miniforge3/bin/:$PATH\\\"\\nconda init zsh\\n\\n# create and use a new env\\nconda create --name tf python=3.9\\nconda activate tf\\n\\n# install tensorflow deps\\nconda install -c apple tensorflow-deps\\n# base tensorflow + metal plugin\\npython -m pip install tensorflow-macos\\npython -m pip install tensorflow-metal\\n\\n# install jupyter, pandas and whatnot\\nconda install -c conda-forge -y pandas jupyter\\n\\nmkdir -p Projects/lab/tfsetup \u0026\u0026 cd Projects/lab/tfsetup\\nrm -rf tokenizers\\ngit clone https://github.com/huggingface/tokenizers\\ncd tokenizers/bindings/python\\n# compile tokenizers - should be pretty fast on your m1\\npip install setuptools_rust\\n# install tokenizers\\npython setup.py install\\n\\n# install transformers using pip\\npip install git+https://github.com/huggingface/transformers\\npip install numpy --upgrade --ignore-installed\\n\\narch -arm64 brew install llvm\\narch -arm64 brew install cmake\\narch -arm64 brew install pkgconfig\\n\\n\\ncd ../../../../../../ # Back to root dir\\n\\n# run the test code\\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\\npython gptjex.py\\n\\n\\n\")), mdx(\"p\", null, \"If you run now you will see errors about MPS not supporting different things:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"RuntimeError: MPS does not support cumsum op with int64 input\\n\")), mdx(\"p\", null, \"To get around this:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"export PYTORCH_ENABLE_MPS_FALLBACK=1\\n\")), mdx(\"h3\", null, \"Testing Install:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"import tensorflow as tf\\n\\nprint(\\\"GPUs: \\\", len(tf.config.experimental.list_physical_devices('GPU')))\\n\")), mdx(\"h3\", null, \"Run the actual code:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"#!/usr/bin/env python3\\n# export PYTORCH_ENABLE_MPS_FALLBACK=1\\n\\nfrom transformers import GPTJForCausalLM, AutoTokenizer\\nimport torch\\nimport os\\nmodel = GPTJForCausalLM.from_pretrained(\\\"EleutherAI/gpt-j-6B\\\")\\nprint(\\\"Created model\\\")\\ntokenizer = AutoTokenizer.from_pretrained(\\\"EleutherAI/gpt-j-6B\\\")\\nprint(\\\"Created tokenizer\\\")\\nprompt = (\\n    \\\"How do I create a thread in GoLang?\\\"\\n)\\n\\ninput_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids\\nprint(\\\"Got input_ids from tokenizer\\\")\\ngen_tokens = model.generate(\\n    input_ids,\\n    do_sample=True,\\n    temperature=0.9,\\n    max_length=2048,\\n    pad_token_id=tokenizer.eos_token_id\\n)\\nprint(\\\"Generated tokens\\\")\\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\\nprint(\\\"Got gen_text from tokenizer.batch_decode\\\\n\\\\n\\\\n\\\")\\nprint(gen_text)\\nos.system(\\\"say done\\\")\\n\\n\")), mdx(\"h3\", null, \"Possible issues:\"), mdx(\"p\", null, \"If you get an error about the device number not being valid from the model.generate() call, try setting the device to new pytorch mps device:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"# Edit the code for happytransformer\\ncode /opt/homebrew/anaconda3/envs/tf2/lib/python3.9/site-packages/happytransformer/happy_generation.py\\n\")), mdx(\"p\", null, \"On line 58 or so there is:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"device_number = detect_cuda_device_number()\\n\")), mdx(\"p\", null, \"I changed this to:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"device_number = torch.device(\\\"mps\\\")\\n\")), mdx(\"p\", null, \"I also had to make sure I had:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"export PYTORCH_ENABLE_MPS_FALLBACK=1\\n\")), mdx(\"h3\", null, \"Other Things I Did That Might Have Helped:\"), mdx(\"p\", null, \"Install nightly.\\nI believe this is what finally worked:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly\\n\\n\")), mdx(\"p\", null, \"Although the documentation (I believe) and what I tried before doing the command above was:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"pip install --pre torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu\\n\")), mdx(\"h3\", null, \"Testing PyTorch + CUDA:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"python -c 'import torch; print torch.cuda.is_available()'\\\\\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","renderedOutput":"\u003ch1\u003eRunning GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook\u003c/h1\u003e\u003cp\u003e\u003cimg src=\"/static/gpt.gif\" alt=\"Running GPT locally on a M1 Macbook\"/\u003e\n\u003cem\u003eActual runtime on my 64GB RAM Apple M1 Max Macbook was 15 minutes. The first answer I got back from this prompt was actually more concise but the answer in this video is actually a better, more thorough one\u003c/em\u003e\u003c/p\u003e\u003ch2\u003eML - CoPilot and ChatGPT\u003c/h2\u003e\u003cp\u003eIt is amazing how helpful GitHub CoPilot / ChatGPT can be sometimes. And I have no previous experience with Machine Learning really. At least past a surface level.\u003c/p\u003e\u003cp\u003eI wanted to learn more and really my goal is to create something like \u003ca href=\"https://github.com/arc53/DocsGPT\"\u003eDocsGPT\u003c/a\u003e which will analyze your codebase\u0026#x27;s code and documentation and be able to provide in depth answers specific to your codebase. I tried some simple tests setup on a GPU instance on AWS but this was going to be very expensive and I of course would prefer to get this all running locally on my M1 Macbook.\u003c/p\u003e\u003ch2\u003ePyTorch has M1 Macbook Support!\u003c/h2\u003e\u003cp\u003eAfter looking around I found that PyTorch JUST recently added support for M1 Macbooks.\u003c/p\u003e\u003cp\u003eBelow is the documentation I used to get it working.\u003c/p\u003e\u003cp\u003eNOTE:\nMost of this is coming from (copied from) this \u003ca href=\"https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/\"\u003eblog post\u003c/a\u003e\nI will be updating the same information and it will be more up to date in the \u003ca href=\"https://github.com/bebrws/gptj6b-happy-on-m1/\"\u003erepository\u003c/a\u003e\nI just had to do a few different things which I document toward the end to get this working on my machine. Most importantly, this was failing to run due to a lack of CUDA support. To fix this I needed to install pytorch-nightly (which can be done a few ways). And then PyTorch doesn\u0026#x27;t supprt the M1 \u0026quot;mps\u0026quot; device so I needed to set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1\nAll of this is documented below.\u003c/p\u003e\u003ch2\u003eSources and What I Followed To Get Here:\u003c/h2\u003e\u003cpre\u003e\u003ccode\u003ehttps://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/\nhttps://github.com/huggingface/transformers/blob/ba0e370dc1713b0ddd9b1be0ac31ef1fdc7bdf76/docs/source/en/model_doc/gptj.mdx?plain=1#L62\n\u003c/code\u003e\u003c/pre\u003e\u003ch2\u003eSetup:\u003c/h2\u003e\u003cp\u003ePreRequirements:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInstall xcode and brew\u003c/li\u003e\u003cli\u003eMiniForge3 (don’t use Anaconda) -\u0026gt; download the arm64 sh from GitHub - \u003ca href=\"https://github.com/conda-forge/miniforge#download\"\u003ehttps://github.com/conda-forge/miniforge#download\u003c/a\u003e\u003c/li\u003e\u003cli\u003eMake sure you have the latest rust installed via rustup\u003c/li\u003e\u003cli\u003eYou will need to have brew installed llvm, cmake, and pkgconfig as well\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eSteps:\u003c/h3\u003e\u003cpre\u003e\u003ccode\u003e# set up conda for fish\nexport PATH=\u0026quot;$HOME/miniforge3/bin/:$PATH\u0026quot;\nconda init zsh\n\n# create and use a new env\nconda create --name tf python=3.9\nconda activate tf\n\n# install tensorflow deps\nconda install -c apple tensorflow-deps\n# base tensorflow + metal plugin\npython -m pip install tensorflow-macos\npython -m pip install tensorflow-metal\n\n# install jupyter, pandas and whatnot\nconda install -c conda-forge -y pandas jupyter\n\nmkdir -p Projects/lab/tfsetup \u0026amp;\u0026amp; cd Projects/lab/tfsetup\nrm -rf tokenizers\ngit clone https://github.com/huggingface/tokenizers\ncd tokenizers/bindings/python\n# compile tokenizers - should be pretty fast on your m1\npip install setuptools_rust\n# install tokenizers\npython setup.py install\n\n# install transformers using pip\npip install git+https://github.com/huggingface/transformers\npip install numpy --upgrade --ignore-installed\n\narch -arm64 brew install llvm\narch -arm64 brew install cmake\narch -arm64 brew install pkgconfig\n\n\ncd ../../../../../../ # Back to root dir\n\n# run the test code\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\npython gptjex.py\n\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you run now you will see errors about MPS not supporting different things:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eRuntimeError: MPS does not support cumsum op with int64 input\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eTo get around this:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eexport PYTORCH_ENABLE_MPS_FALLBACK=1\n\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003eTesting Install:\u003c/h3\u003e\u003cpre\u003e\u003ccode\u003eimport tensorflow as tf\n\nprint(\u0026quot;GPUs: \u0026quot;, len(tf.config.experimental.list_physical_devices(\u0026#x27;GPU\u0026#x27;)))\n\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003eRun the actual code:\u003c/h3\u003e\u003cpre\u003e\u003ccode\u003e#!/usr/bin/env python3\n# export PYTORCH_ENABLE_MPS_FALLBACK=1\n\nfrom transformers import GPTJForCausalLM, AutoTokenizer\nimport torch\nimport os\nmodel = GPTJForCausalLM.from_pretrained(\u0026quot;EleutherAI/gpt-j-6B\u0026quot;)\nprint(\u0026quot;Created model\u0026quot;)\ntokenizer = AutoTokenizer.from_pretrained(\u0026quot;EleutherAI/gpt-j-6B\u0026quot;)\nprint(\u0026quot;Created tokenizer\u0026quot;)\nprompt = (\n    \u0026quot;How do I create a thread in GoLang?\u0026quot;\n)\n\ninput_ids = tokenizer(prompt, return_tensors=\u0026quot;pt\u0026quot;).input_ids\nprint(\u0026quot;Got input_ids from tokenizer\u0026quot;)\ngen_tokens = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.9,\n    max_length=2048,\n    pad_token_id=tokenizer.eos_token_id\n)\nprint(\u0026quot;Generated tokens\u0026quot;)\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(\u0026quot;Got gen_text from tokenizer.batch_decode\\n\\n\\n\u0026quot;)\nprint(gen_text)\nos.system(\u0026quot;say done\u0026quot;)\n\n\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003ePossible issues:\u003c/h3\u003e\u003cp\u003eIf you get an error about the device number not being valid from the model.generate() call, try setting the device to new pytorch mps device:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e# Edit the code for happytransformer\ncode /opt/homebrew/anaconda3/envs/tf2/lib/python3.9/site-packages/happytransformer/happy_generation.py\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOn line 58 or so there is:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003edevice_number = detect_cuda_device_number()\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eI changed this to:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003edevice_number = torch.device(\u0026quot;mps\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eI also had to make sure I had:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eexport PYTORCH_ENABLE_MPS_FALLBACK=1\n\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003eOther Things I Did That Might Have Helped:\u003c/h3\u003e\u003cp\u003eInstall nightly.\nI believe this is what finally worked:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003econda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly\n\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAlthough the documentation (I believe) and what I tried before doing the command above was:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003epip install --pre torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003eTesting PyTorch + CUDA:\u003c/h3\u003e\u003cpre\u003e\u003ccode\u003epython -c \u0026#x27;import torch; print torch.cuda.is_available()\u0026#x27;\\\n\u003c/code\u003e\u003c/pre\u003e","scope":{"slug":"running-GPTJ-on-M1-Macbook","title":"Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook","date":"2023-02-26","author":"bebrws","tags":["Machine Learning","AI","ML","GPTJ","GPT","GPT-3","GPT-J","GPT-J-6B","Happy Transformer","HuggingFace"]}}},"__N_SSG":true},"page":"/posts/[post]","query":{"post":"running-GPTJ-on-M1-Macbook"},"buildId":"jhgFwLEJbxGrw0_D-f_Gp","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>