{"pageProps":{"title":"Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook","dateString":"2023-02-26","slug":"running-GPTJ-on-M1-Macbook","description":"","tags":["Machine Learning","AI","ML","GPTJ","GPT","GPT-3","GPT-J","GPT-J-6B","Happy Transformer","HuggingFace"],"author":"bebrws","source":{"compiledSource":"\"use strict\";\n\nvar _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar layoutProps = {};\nvar MDXLayout = \"wrapper\";\n\nfunction MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/static/gpt.gif\",\n    \"alt\": \"Running GPT locally on a M1 Macbook\"\n  }), \"\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Actual runtime on my 64GB RAM Apple M1 Max Macbook was 15 minutes. The first answer I got back from this prompt was actually more concise but the answer in this video is actually a better, more thorough one\")), mdx(\"h2\", null, \"ML - CoPilot and ChatGPT\"), mdx(\"p\", null, \"It is amazing how helpful GitHub CoPilot / ChatGPT can be sometimes. And I have no previous experience with Machine Learning really. At least past a surface level.\"), mdx(\"p\", null, \"I wanted to learn more and really my goal is to create something like \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/arc53/DocsGPT\"\n  }, \"DocsGPT\"), \" which will analyze your codebase's code and documentation and be able to provide in depth answers specific to your codebase. I tried some simple tests setup on a GPU instance on AWS but this was going to be very expensive and I of course would prefer to get this all running locally on my M1 Macbook.\"), mdx(\"h2\", null, \"PyTorch has M1 Macbook Support!\"), mdx(\"p\", null, \"After looking around I found that PyTorch JUST recently added support for M1 Macbooks.\"), mdx(\"p\", null, \"Below is the documentation I used to get it working.\"), mdx(\"p\", null, \"NOTE:\\nMost of this is coming from (copied from) this \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/\"\n  }, \"blog post\"), \"\\nI will be updating the same information and it will be more up to date in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/bebrws/gptj6b-happy-on-m1/\"\n  }, \"repository\"), \"\\nI just had to do a few different things which I document toward the end to get this working on my machine. Most importantly, this was failing to run due to a lack of CUDA support. To fix this I needed to install pytorch-nightly (which can be done a few ways). And then PyTorch doesn't supprt the M1 \\\"mps\\\" device so I needed to set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1\\nAll of this is documented below.\"), mdx(\"h2\", null, \"Sources and What I Followed To Get Here:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/\\nhttps://github.com/huggingface/transformers/blob/ba0e370dc1713b0ddd9b1be0ac31ef1fdc7bdf76/docs/source/en/model_doc/gptj.mdx?plain=1#L62\\n\")), mdx(\"h2\", null, \"Setup:\"), mdx(\"p\", null, \"PreRequirements:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Install xcode and brew\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"MiniForge3 (don\\u2019t use Anaconda) -> download the arm64 sh from GitHub - \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/conda-forge/miniforge#download\"\n  }, \"https://github.com/conda-forge/miniforge#download\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Make sure you have the latest rust installed via rustup\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You will need to have brew installed llvm, cmake, and pkgconfig as well\")), mdx(\"h3\", null, \"Steps:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"# set up conda for fish\\nexport PATH=\\\"$HOME/miniforge3/bin/:$PATH\\\"\\nconda init zsh\\n\\n# create and use a new env\\nconda create --name tf python=3.9\\nconda activate tf\\n\\n# install tensorflow deps\\nconda install -c apple tensorflow-deps\\n# base tensorflow + metal plugin\\npython -m pip install tensorflow-macos\\npython -m pip install tensorflow-metal\\n\\n# install jupyter, pandas and whatnot\\nconda install -c conda-forge -y pandas jupyter\\n\\nmkdir -p Projects/lab/tfsetup && cd Projects/lab/tfsetup\\nrm -rf tokenizers\\ngit clone https://github.com/huggingface/tokenizers\\ncd tokenizers/bindings/python\\n# compile tokenizers - should be pretty fast on your m1\\npip install setuptools_rust\\n# install tokenizers\\npython setup.py install\\n\\n# install transformers using pip\\npip install git+https://github.com/huggingface/transformers\\npip install numpy --upgrade --ignore-installed\\n\\narch -arm64 brew install llvm\\narch -arm64 brew install cmake\\narch -arm64 brew install pkgconfig\\n\\n\\ncd ../../../../../../ # Back to root dir\\n\\n# run the test code\\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\\npython gptjex.py\\n\\n\\n\")), mdx(\"p\", null, \"If you run now you will see errors about MPS not supporting different things:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"RuntimeError: MPS does not support cumsum op with int64 input\\n\")), mdx(\"p\", null, \"To get around this:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"export PYTORCH_ENABLE_MPS_FALLBACK=1\\n\")), mdx(\"h3\", null, \"Testing Install:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"import tensorflow as tf\\n\\nprint(\\\"GPUs: \\\", len(tf.config.experimental.list_physical_devices('GPU')))\\n\")), mdx(\"h3\", null, \"Run the actual code:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"#!/usr/bin/env python3\\n# export PYTORCH_ENABLE_MPS_FALLBACK=1\\n\\nfrom transformers import GPTJForCausalLM, AutoTokenizer\\nimport torch\\nimport os\\nmodel = GPTJForCausalLM.from_pretrained(\\\"EleutherAI/gpt-j-6B\\\")\\nprint(\\\"Created model\\\")\\ntokenizer = AutoTokenizer.from_pretrained(\\\"EleutherAI/gpt-j-6B\\\")\\nprint(\\\"Created tokenizer\\\")\\nprompt = (\\n    \\\"How do I create a thread in GoLang?\\\"\\n)\\n\\ninput_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids\\nprint(\\\"Got input_ids from tokenizer\\\")\\ngen_tokens = model.generate(\\n    input_ids,\\n    do_sample=True,\\n    temperature=0.9,\\n    max_length=2048,\\n    pad_token_id=tokenizer.eos_token_id\\n)\\nprint(\\\"Generated tokens\\\")\\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\\nprint(\\\"Got gen_text from tokenizer.batch_decode\\\\n\\\\n\\\\n\\\")\\nprint(gen_text)\\nos.system(\\\"say done\\\")\\n\\n\")), mdx(\"h3\", null, \"Possible issues:\"), mdx(\"p\", null, \"If you get an error about the device number not being valid from the model.generate() call, try setting the device to new pytorch mps device:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"# Edit the code for happytransformer\\ncode /opt/homebrew/anaconda3/envs/tf2/lib/python3.9/site-packages/happytransformer/happy_generation.py\\n\")), mdx(\"p\", null, \"On line 58 or so there is:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"device_number = detect_cuda_device_number()\\n\")), mdx(\"p\", null, \"I changed this to:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"device_number = torch.device(\\\"mps\\\")\\n\")), mdx(\"p\", null, \"I also had to make sure I had:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"export PYTORCH_ENABLE_MPS_FALLBACK=1\\n\")), mdx(\"h3\", null, \"Other Things I Did That Might Have Helped:\"), mdx(\"p\", null, \"Install nightly.\\nI believe this is what finally worked:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly\\n\\n\")), mdx(\"p\", null, \"Although the documentation (I believe) and what I tried before doing the command above was:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"pip install --pre torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu\\n\")), mdx(\"h3\", null, \"Testing PyTorch + CUDA:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"python -c 'import torch; print torch.cuda.is_available()'\\\\\\n\")));\n}\n\n;\nMDXContent.isMDXComponent = true;","renderedOutput":"<h1>Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook</h1><p><img src=\"/static/gpt.gif\" alt=\"Running GPT locally on a M1 Macbook\"/>\n<em>Actual runtime on my 64GB RAM Apple M1 Max Macbook was 15 minutes. The first answer I got back from this prompt was actually more concise but the answer in this video is actually a better, more thorough one</em></p><h2>ML - CoPilot and ChatGPT</h2><p>It is amazing how helpful GitHub CoPilot / ChatGPT can be sometimes. And I have no previous experience with Machine Learning really. At least past a surface level.</p><p>I wanted to learn more and really my goal is to create something like <a href=\"https://github.com/arc53/DocsGPT\">DocsGPT</a> which will analyze your codebase&#x27;s code and documentation and be able to provide in depth answers specific to your codebase. I tried some simple tests setup on a GPU instance on AWS but this was going to be very expensive and I of course would prefer to get this all running locally on my M1 Macbook.</p><h2>PyTorch has M1 Macbook Support!</h2><p>After looking around I found that PyTorch JUST recently added support for M1 Macbooks.</p><p>Below is the documentation I used to get it working.</p><p>NOTE:\nMost of this is coming from (copied from) this <a href=\"https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/\">blog post</a>\nI will be updating the same information and it will be more up to date in the <a href=\"https://github.com/bebrws/gptj6b-happy-on-m1/\">repository</a>\nI just had to do a few different things which I document toward the end to get this working on my machine. Most importantly, this was failing to run due to a lack of CUDA support. To fix this I needed to install pytorch-nightly (which can be done a few ways). And then PyTorch doesn&#x27;t supprt the M1 &quot;mps&quot; device so I needed to set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1\nAll of this is documented below.</p><h2>Sources and What I Followed To Get Here:</h2><pre><code>https://lazycoder.ro/posts/using-gpt-neo-on-m1-mac/\nhttps://github.com/huggingface/transformers/blob/ba0e370dc1713b0ddd9b1be0ac31ef1fdc7bdf76/docs/source/en/model_doc/gptj.mdx?plain=1#L62\n</code></pre><h2>Setup:</h2><p>PreRequirements:</p><ul><li>Install xcode and brew</li><li>MiniForge3 (don’t use Anaconda) -&gt; download the arm64 sh from GitHub - <a href=\"https://github.com/conda-forge/miniforge#download\">https://github.com/conda-forge/miniforge#download</a></li><li>Make sure you have the latest rust installed via rustup</li><li>You will need to have brew installed llvm, cmake, and pkgconfig as well</li></ul><h3>Steps:</h3><pre><code># set up conda for fish\nexport PATH=&quot;$HOME/miniforge3/bin/:$PATH&quot;\nconda init zsh\n\n# create and use a new env\nconda create --name tf python=3.9\nconda activate tf\n\n# install tensorflow deps\nconda install -c apple tensorflow-deps\n# base tensorflow + metal plugin\npython -m pip install tensorflow-macos\npython -m pip install tensorflow-metal\n\n# install jupyter, pandas and whatnot\nconda install -c conda-forge -y pandas jupyter\n\nmkdir -p Projects/lab/tfsetup &amp;&amp; cd Projects/lab/tfsetup\nrm -rf tokenizers\ngit clone https://github.com/huggingface/tokenizers\ncd tokenizers/bindings/python\n# compile tokenizers - should be pretty fast on your m1\npip install setuptools_rust\n# install tokenizers\npython setup.py install\n\n# install transformers using pip\npip install git+https://github.com/huggingface/transformers\npip install numpy --upgrade --ignore-installed\n\narch -arm64 brew install llvm\narch -arm64 brew install cmake\narch -arm64 brew install pkgconfig\n\n\ncd ../../../../../../ # Back to root dir\n\n# run the test code\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\npython gptjex.py\n\n\n</code></pre><p>If you run now you will see errors about MPS not supporting different things:</p><pre><code>RuntimeError: MPS does not support cumsum op with int64 input\n</code></pre><p>To get around this:</p><pre><code>export PYTORCH_ENABLE_MPS_FALLBACK=1\n</code></pre><h3>Testing Install:</h3><pre><code>import tensorflow as tf\n\nprint(&quot;GPUs: &quot;, len(tf.config.experimental.list_physical_devices(&#x27;GPU&#x27;)))\n</code></pre><h3>Run the actual code:</h3><pre><code>#!/usr/bin/env python3\n# export PYTORCH_ENABLE_MPS_FALLBACK=1\n\nfrom transformers import GPTJForCausalLM, AutoTokenizer\nimport torch\nimport os\nmodel = GPTJForCausalLM.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\nprint(&quot;Created model&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\nprint(&quot;Created tokenizer&quot;)\nprompt = (\n    &quot;How do I create a thread in GoLang?&quot;\n)\n\ninput_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids\nprint(&quot;Got input_ids from tokenizer&quot;)\ngen_tokens = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.9,\n    max_length=2048,\n    pad_token_id=tokenizer.eos_token_id\n)\nprint(&quot;Generated tokens&quot;)\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(&quot;Got gen_text from tokenizer.batch_decode\\n\\n\\n&quot;)\nprint(gen_text)\nos.system(&quot;say done&quot;)\n\n</code></pre><h3>Possible issues:</h3><p>If you get an error about the device number not being valid from the model.generate() call, try setting the device to new pytorch mps device:</p><pre><code># Edit the code for happytransformer\ncode /opt/homebrew/anaconda3/envs/tf2/lib/python3.9/site-packages/happytransformer/happy_generation.py\n</code></pre><p>On line 58 or so there is:</p><pre><code>device_number = detect_cuda_device_number()\n</code></pre><p>I changed this to:</p><pre><code>device_number = torch.device(&quot;mps&quot;)\n</code></pre><p>I also had to make sure I had:</p><pre><code>export PYTORCH_ENABLE_MPS_FALLBACK=1\n</code></pre><h3>Other Things I Did That Might Have Helped:</h3><p>Install nightly.\nI believe this is what finally worked:</p><pre><code>conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly\n\n</code></pre><p>Although the documentation (I believe) and what I tried before doing the command above was:</p><pre><code>pip install --pre torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n</code></pre><h3>Testing PyTorch + CUDA:</h3><pre><code>python -c &#x27;import torch; print torch.cuda.is_available()&#x27;\\\n</code></pre>","scope":{"slug":"running-GPTJ-on-M1-Macbook","title":"Running GPTJ (EleutherAI/gpt-j-6B) on a M1 Macbook","date":"2023-02-26","author":"bebrws","tags":["Machine Learning","AI","ML","GPTJ","GPT","GPT-3","GPT-J","GPT-J-6B","Happy Transformer","HuggingFace"]}}},"__N_SSG":true}